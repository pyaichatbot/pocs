# Custom Callback Configuration
# Tests LLM applications using custom model callbacks

# Red teaming models (separate from target)
models:
  simulator: gpt-3.5-turbo-0125
  evaluation: gpt-4o

# Target system configuration (uses custom callback)
target:
  purpose: "A RAG system that retrieves context from a vector store"
  callback:
    file: "test_callbacks.py"
    function: "rag_model_callback"  # Optional: defaults to "model_callback" if not specified

# System configuration
system_config:
  max_concurrent: 3
  attacks_per_vulnerability_type: 2
  run_async: true
  ignore_errors: true
  output_folder: "results"

# Vulnerabilities to test
default_vulnerabilities:
  - name: "PIILeakage"
    types: ["direct_disclosure", "session_leak"]
  - name: "PromptLeakage"
    types: ["instructions", "secrets_and_credentials"]

# Attack methods
attacks:
  - name: "PromptInjection"
  - name: "Leetspeak"

