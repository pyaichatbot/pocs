"""
LLM client abstractions.

This module provides a unified interface for interacting with large
language models (LLMs) from different providers.  It currently
supports Azure OpenAI and Anthropic via the ``openai`` and
``anthropic`` Python packages respectively.  If the required
libraries or API keys are not present, the client falls back to a
no‑op implementation that simply returns concatenated context.

The ``generate_answer`` function is used by the search endpoint to
produce a conversational response given a user query and supporting
context from the knowledge base.  The exact prompt format can be
modified to suit your application’s tone and style.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List

from .config import Settings


@dataclass
class DocumentChunk:
    """Represents a single chunk of a document for context assembly."""
    doc_id: str
    chunk_id: str
    path: str
    content: str
    metadata: dict


class LLMClient:
    """Unified client for multiple LLM providers."""

    def __init__(self, settings: Settings):
        self.settings = settings
        self.provider = settings.llm_provider
        # Attempt to import provider SDKs; if unavailable, we'll use fallback
        self.openai = None
        self.anthropic = None
        if self.provider == "azure":
            try:
                import openai  # type: ignore
                self.openai = openai
                # Configure Azure endpoint
                self.openai.api_type = "azure"
                self.openai.api_base = settings.azure_endpoint
                self.openai.api_key = settings.azure_api_key
                self.openai.api_version = "2023-05-15"  # Example version
            except Exception:
                self.openai = None
        elif self.provider == "anthropic":
            try:
                import anthropic  # type: ignore
                # Use Anthropic SDK v0.18+ (messages API)
                self.anthropic = anthropic.Anthropic(api_key=settings.anthropic_api_key)
            except Exception:
                self.anthropic = None

    def is_available(self) -> bool:
        """Return True if the configured provider is available."""
        if self.provider == "azure":
            return self.openai is not None
        if self.provider == "anthropic":
            return self.anthropic is not None
        return False

    def generate_answer(self, query: str, contexts: List[DocumentChunk]) -> str:
        """Generate an answer using the configured LLM provider.

        Args:
            query: The user question.
            contexts: List of document chunks providing support.
        Returns:
            A string answer generated by the LLM, or a concatenation of
            contexts if no LLM is available.
        """
        # Assemble context text
        context_text = "\n\n".join(chunk.content for chunk in contexts)
        # If no provider is configured or available, return fallback
        if not self.is_available():
            # Fallback: return context and question when LLM is unavailable
            # Callers can choose to raise an exception if LLM is required
            return (
                "[LLM unavailable] Answer could not be generated.\n"
                "Context:\n" + context_text + "\n\nQuestion:\n" + query
            )
        if self.provider == "azure":
            # Construct a prompt for Azure OpenAI Chat API
            messages = [
                {"role": "system", "content": "You are a knowledgeable assistant."},
                {
                    "role": "user",
                    "content": f"Answer the following question based only on the provided context.\n\n"
                    f"Context:\n{context_text}\n\nQuestion: {query}",
                },
            ]
            try:
                response = self.openai.ChatCompletion.create(
                    engine=self.settings.azure_deployment_name,
                    messages=messages,
                    temperature=0.0,
                    max_tokens=512,
                )
                return response["choices"][0]["message"]["content"].strip()
            except Exception:
                return (
                    "[Azure OpenAI call failed] Answer could not be generated.\n"
                    "Context:\n" + context_text + "\n\nQuestion:\n" + query
                )
        if self.provider == "anthropic":
            try:
                # Use Anthropic Messages API (v0.18+)
                message = self.anthropic.messages.create(
                    model=self.settings.anthropic_model,
                    max_tokens=512,
                    temperature=0.0,
                    messages=[
                        {
                            "role": "user",
                            "content": (
                                f"Answer the following question based only on the provided context.\n\n"
                                f"Context:\n{context_text}\n\nQuestion: {query}"
                            ),
                        }
                    ],
                )
                return message.content[0].text.strip()
            except Exception as exc:
                return (
                    f"[Anthropic call failed] Answer could not be generated.\n"
                    f"Error: {exc}\n\n"
                    f"Context:\n{context_text}\n\nQuestion:\n{query}"
                )
        # Should not reach here
        return (
            "[Unsupported LLM provider] Answer could not be generated.\n"
            "Context:\n" + context_text + "\n\nQuestion:\n" + query
        )