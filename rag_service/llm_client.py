"""
LLM client abstractions.

This module provides a unified interface for interacting with large
language models (LLMs) from different providers.  It currently
supports Azure OpenAI and Anthropic via the ``openai`` and
``anthropic`` Python packages respectively.  If the required
libraries or API keys are not present, the client falls back to a
no‑op implementation that simply returns concatenated context.

The ``generate_answer`` function is used by the search endpoint to
produce a conversational response given a user query and supporting
context from the knowledge base.  The exact prompt format can be
modified to suit your application’s tone and style.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List
import json

from .config import Settings
from .utils.toon_encoder import (
    encode_contexts_hybrid,
    encode_contexts_flattened,
    encode_contexts_to_toon,
    choose_best_format,
)
from .utils.logging import get_logger, log_event

logger = get_logger(__name__)


@dataclass
class DocumentChunk:
    """Represents a single chunk of a document for context assembly."""
    doc_id: str
    chunk_id: str
    path: str
    content: str
    metadata: dict


class LLMClient:
    """Unified client for multiple LLM providers."""

    def __init__(self, settings: Settings):
        self.settings = settings
        self.provider = settings.llm_provider
        # Attempt to import provider SDKs; if unavailable, we'll use fallback
        self.openai = None
        self.anthropic = None
        if self.provider == "azure":
            try:
                import openai  # type: ignore
                self.openai = openai
                # Configure Azure endpoint
                self.openai.api_type = "azure"
                self.openai.api_base = settings.azure_endpoint
                self.openai.api_key = settings.azure_api_key
                self.openai.api_version = "2023-05-15"  # Example version
            except Exception:
                self.openai = None
        elif self.provider == "anthropic":
            try:
                import anthropic  # type: ignore
                # Use Anthropic SDK v0.18+ (messages API)
                self.anthropic = anthropic.Anthropic(api_key=settings.anthropic_api_key)
            except Exception:
                self.anthropic = None

    def is_available(self) -> bool:
        """Return True if the configured provider is available."""
        if self.provider == "azure":
            return self.openai is not None
        if self.provider == "anthropic":
            return self.anthropic is not None
        return False

    def generate_answer(self, query: str, contexts: List[DocumentChunk]) -> str:
        """Generate an answer using the configured LLM provider.

        Args:
            query: The user question.
            contexts: List of document chunks providing support.
        Returns:
            A string answer generated by the LLM, or a concatenation of
            contexts if no LLM is available.
        """
        # Format context based on configuration
        format_type = getattr(self.settings, "use_llm_token_format", "hybrid").lower()
        context_text, format_used, token_count = self._format_context(contexts, format_type)
        
        # Log format selection and token count
        log_event(
            logger,
            "llm_context_formatted",
            format=format_used,
            requested_format=format_type,
            contexts_count=len(contexts),
            token_count=token_count,
            context_length=len(context_text),
        )
        # If no provider is configured or available, return fallback
        if not self.is_available():
            # Fallback: return context and question when LLM is unavailable
            # Callers can choose to raise an exception if LLM is required
            return (
                "[LLM unavailable] Answer could not be generated.\n"
                "Context:\n" + context_text + "\n\nQuestion:\n" + query
            )
        if self.provider == "azure":
            # Construct a prompt for Azure OpenAI Chat API
            messages = [
                {"role": "system", "content": "You are a knowledgeable assistant."},
                {
                    "role": "user",
                    "content": f"Answer the following question based only on the provided context.\n\n"
                    f"Context:\n{context_text}\n\nQuestion: {query}",
                },
            ]
            try:
                response = self.openai.ChatCompletion.create(
                    engine=self.settings.azure_deployment_name,
                    messages=messages,
                    temperature=0.0,
                    max_tokens=getattr(self.settings, "llm_max_tokens", 4096),
                )
                return response["choices"][0]["message"]["content"].strip()
            except Exception:
                return (
                    "[Azure OpenAI call failed] Answer could not be generated.\n"
                    "Context:\n" + context_text + "\n\nQuestion:\n" + query
                )
        if self.provider == "anthropic":
            try:
                # Use Anthropic Messages API (v0.18+)
                message = self.anthropic.messages.create(
                    model=self.settings.anthropic_model,
                    max_tokens=getattr(self.settings, "llm_max_tokens", 4096),
                    temperature=0.0,
                    messages=[
                        {
                            "role": "user",
                            "content": (
                                f"Answer the following question based only on the provided context.\n\n"
                                f"Context:\n{context_text}\n\nQuestion: {query}"
                            ),
                        }
                    ],
                )
                return message.content[0].text.strip()
            except Exception as exc:
                return (
                    f"[Anthropic call failed] Answer could not be generated.\n"
                    f"Error: {exc}\n\n"
                    f"Context:\n{context_text}\n\nQuestion:\n{query}"
                )
        # Should not reach here
        return (
            "[Unsupported LLM provider] Answer could not be generated.\n"
            "Context:\n" + context_text + "\n\nQuestion:\n" + query
        )

    def _format_context(
        self, contexts: List[DocumentChunk], format_type: str
    ) -> tuple[str, str, int]:
        """Format contexts based on configured format type.
        
        Args:
            contexts: List of document chunks.
            format_type: Format type ("plain", "toon", "hybrid", "json").
        
        Returns:
            Tuple of (formatted_text, format_used, estimated_token_count).
        """
        if not contexts:
            return "", "empty", 0
        
        # Single context - always use plain text for simplicity
        if len(contexts) == 1:
            context_text = contexts[0].content
            token_count = self._estimate_tokens(context_text)
            return context_text, "plain", token_count
        
        # Multiple contexts - use configured format
        try:
            if format_type == "plain":
                # Plain text concatenation (backward compatible)
                context_text = "\n\n".join(chunk.content for chunk in contexts)
                token_count = self._estimate_tokens(context_text)
                return context_text, "plain", token_count
            
            elif format_type == "toon":
                # Fully flattened TOON format
                context_text = encode_contexts_flattened(contexts)
                token_count = self._estimate_tokens(context_text)
                # Wrap in code block for clarity
                context_text = f"```toon\n{context_text}\n```"
                return context_text, "toon", token_count
            
            elif format_type == "hybrid":
                # Hybrid TOON + JSON (recommended)
                context_text = encode_contexts_hybrid(contexts, include_metadata=True)
                token_count = self._estimate_tokens(context_text)
                # Wrap in code block for clarity
                context_text = f"```toon\n{context_text}\n```"
                return context_text, "hybrid", token_count
            
            elif format_type == "json":
                # JSON format (structured, for debugging)
                context_data = [
                    {
                        "path": chunk.path,
                        "content": chunk.content[:500],  # Truncate
                        "metadata": chunk.metadata or {},
                    }
                    for chunk in contexts
                ]
                context_text = json.dumps({"contexts": context_data}, indent=2)
                token_count = self._estimate_tokens(context_text)
                return context_text, "json", token_count
            
            else:
                # Unknown format - use smart selection
                recommended = choose_best_format(contexts)
                log_event(
                    logger,
                    "llm_format_unknown_using_smart",
                    requested=format_type,
                    recommended=recommended,
                )
                
                if recommended == "hybrid":
                    context_text = encode_contexts_hybrid(contexts, include_metadata=True)
                    context_text = f"```toon\n{context_text}\n```"
                elif recommended == "toon":
                    context_text = encode_contexts_flattened(contexts)
                    context_text = f"```toon\n{context_text}\n```"
                else:
                    # Fallback to plain
                    context_text = "\n\n".join(chunk.content for chunk in contexts)
                
                token_count = self._estimate_tokens(context_text)
                return context_text, recommended, token_count
        
        except Exception as exc:
            # Fallback to plain text on any error
            log_event(
                logger,
                "llm_format_error_fallback",
                format=format_type,
                error=str(exc),
                exc_info=True,
            )
            context_text = "\n\n".join(chunk.content for chunk in contexts)
            token_count = self._estimate_tokens(context_text)
            return context_text, "plain", token_count

    def _estimate_tokens(self, text: str) -> int:
        """Estimate token count for text.
        
        Uses tiktoken if available for accurate counts, otherwise falls back
        to character-based estimation.
        
        Args:
            text: Text to count tokens for.
        
        Returns:
            Estimated token count.
        """
        try:
            import tiktoken
            # Try to use model-specific tokenizer
            model = getattr(self.settings, "anthropic_model", None) or getattr(
                self.settings, "azure_deployment_name", None
            )
            
            # Map models to tokenizers
            if model and ("gpt-4" in model.lower() or "gpt-3.5" in model.lower()):
                encoding = tiktoken.encoding_for_model("gpt-4")
            elif model and ("claude" in model.lower() or "sonnet" in model.lower()):
                # Claude uses cl100k_base (same as GPT-4)
                encoding = tiktoken.get_encoding("cl100k_base")
            else:
                # Default to cl100k_base
                encoding = tiktoken.get_encoding("cl100k_base")
            
            return len(encoding.encode(text))
        
        except ImportError:
            # Fallback to character-based estimation
            # GPT-style tokenizers: ~0.25 tokens per character
            # This is a rough estimate
            return int(len(text) * 0.25)
        
        except Exception as exc:
            log_event(
                logger,
                "llm_token_count_error",
                error=str(exc),
                fallback=True,
            )
            # Fallback to character-based estimation
            return int(len(text) * 0.25)