# TOON Format Integration - Demo Guide

This guide provides step-by-step instructions for demonstrating the TOON format integration in the RAG service.

## Prerequisites

1. **Docker services running**:
   ```bash
   cd rag_service
   docker-compose ps
   ```
   Should show `rag-service` and `rag-chromadb` as "Up" and healthy.

2. **Service accessible**:
   ```bash
   curl http://localhost:8000/health
   ```
   Should return: `{"status": "healthy", ...}`

## Demo Script - Quick Version (5 minutes)

### Step 1: Verify Service Status (30 seconds)

```bash
cd rag_service

# Check services are running
docker-compose ps

# Check health
curl -s http://localhost:8000/health | python3 -m json.tool

# Check current format configuration
docker-compose exec -T rag-service env | grep USE_LLM_TOKEN_FORMAT
```

**Expected Output:**
```
USE_LLM_TOKEN_FORMAT=hybrid
```

**Talking Point**: "The service is configured with `hybrid` format, which provides the best balance of token savings and metadata preservation."

---

### Step 2: Run Comprehensive Format Comparison Test (2 minutes)

```bash
# Copy test script to container (if not already there)
docker cp test_toon_docker_integration.py rag-service:/app/test_toon_docker_integration.py

# Run comprehensive test
docker-compose exec -T rag-service python test_toon_docker_integration.py
```

**What to Highlight:**
- âœ… **Small contexts**: Show that TOON adds overhead (expected - structure overhead)
- âœ… **Medium contexts**: Show moderate savings
- âœ… **Large contexts**: Show **44%+ token savings** - this is the key benefit!
- âœ… **All formats working**: plain, json, toon, hybrid

**Talking Points**:
- "For small contexts, TOON adds overhead due to structure, so we automatically use plain text."
- "For large contexts (10+ chunks), TOON provides 44%+ token savings."
- "The hybrid format provides a good balance - TOON for content, JSON for metadata."

---

### Step 3: Show Real-Time Format Usage (1 minute)

```bash
# In one terminal, watch logs in real-time
docker-compose logs -f rag-service | grep --line-buffered "llm_context_formatted"

# In another terminal, make a test query
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "What is authentication?"}'
```

**Expected Output in logs:**
```json
{
  "timestamp": "2025-11-06T05:20:00",
  "service": "markdown-rag",
  "event": "llm_context_formatted",
  "format": "hybrid",
  "requested_format": "hybrid",
  "contexts_count": 5,
  "token_count": 320,
  "context_length": 1280
}
```

**Talking Points**:
- "The service logs every format usage with token counts."
- "You can monitor token usage in real-time."
- "The format is automatically selected based on context size."

---

### Step 4: Compare Different Formats (1.5 minutes)

```bash
# Test with different formats
echo "=== Testing PLAIN format ==="
docker-compose exec -T rag-service python -c "
from rag_service.llm_client import LLMClient, DocumentChunk
from rag_service.config import Settings

settings = Settings()
settings.use_llm_token_format = 'plain'
client = LLMClient(settings)

contexts = [
    DocumentChunk(
        doc_id='test.md',
        chunk_id='test.md:0',
        path='test.md',
        content='This is a test document chunk with authentication information. ' * 10,
        metadata={'score': 0.85, 'repo_url': 'https://example.com', 'provider': 'gitlab'}
    ) for _ in range(5)
]

text, fmt, tokens = client._format_context(contexts, 'plain')
print(f'Format: {fmt}, Tokens: {tokens}, Length: {len(text)}')
"

echo ""
echo "=== Testing HYBRID format ==="
docker-compose exec -T rag-service python -c "
from rag_service.llm_client import LLMClient, DocumentChunk
from rag_service.config import Settings

settings = Settings()
settings.use_llm_token_format = 'hybrid'
client = LLMClient(settings)

contexts = [
    DocumentChunk(
        doc_id='test.md',
        chunk_id='test.md:0',
        path='test.md',
        content='This is a test document chunk with authentication information. ' * 10,
        metadata={'score': 0.85, 'repo_url': 'https://example.com', 'provider': 'gitlab'}
    ) for _ in range(5)
]

text, fmt, tokens = client._format_context(contexts, 'hybrid')
print(f'Format: {fmt}, Tokens: {tokens}, Length: {len(text)}')
savings = ((320 - tokens) / 320 * 100) if tokens < 320 else 0
print(f'Estimated savings vs plain: {savings:.1f}%')
"
```

**Talking Points**:
- "You can see the token count difference between formats."
- "Hybrid format typically saves 30-50% tokens for medium to large contexts."

---

## Demo Script - Full Version (10 minutes)

### Additional Steps for Full Demo

### Step 5: Show Format Output Examples (2 minutes)

```bash
# Show actual format outputs
docker-compose exec -T rag-service python -c "
from rag_service.llm_client import LLMClient, DocumentChunk
from rag_service.config import Settings

settings = Settings()
client = LLMClient(settings)

contexts = [
    DocumentChunk(
        doc_id='docs/auth.md',
        chunk_id='docs/auth.md:0',
        path='docs/auth.md',
        content='Authentication is the process of verifying user identity.',
        metadata={'score': 0.85, 'repo_url': 'https://example.com', 'provider': 'gitlab'}
    ),
    DocumentChunk(
        doc_id='docs/api.md',
        chunk_id='docs/api.md:0',
        path='docs/api.md',
        content='API endpoints require authentication tokens.',
        metadata={'score': 0.82, 'repo_url': 'https://example.com', 'provider': 'gitlab'}
    )
]

print('=== PLAIN FORMAT ===')
text, _, tokens = client._format_context(contexts, 'plain')
print(text[:300])
print(f'Tokens: {tokens}')
print()

print('=== TOON FORMAT ===')
text, _, tokens = client._format_context(contexts, 'toon')
print(text[:300])
print(f'Tokens: {tokens}')
print()

print('=== HYBRID FORMAT ===')
text, _, tokens = client._format_context(contexts, 'hybrid')
print(text[:300])
print(f'Tokens: {tokens}')
"
```

**Talking Points**:
- "Plain format is simple concatenation - easy to read but verbose."
- "TOON format uses a compact tabular structure - much more efficient."
- "Hybrid format combines TOON content with JSON metadata - best of both worlds."

---

### Step 6: Show Token Savings Calculation (1 minute)

```bash
# Calculate and display savings
docker-compose exec -T rag-service python -c "
from rag_service.utils.toon_encoder import compare_formats
from rag_service.llm_client import DocumentChunk

# Create test contexts
contexts = [
    DocumentChunk(
        doc_id=f'docs/doc{i}.md',
        chunk_id=f'docs/doc{i}.md:0',
        path=f'docs/doc{i}.md',
        content='This is a longer document chunk with detailed information. ' * 15,
        metadata={'score': 0.85 - (i * 0.02), 'repo_url': 'https://example.com', 'provider': 'gitlab'}
    ) for i in range(10)
]

comparison = compare_formats(contexts)
plain = comparison['plain_text']['tokens']
toon = comparison['toon_format']['tokens']
hybrid = comparison['hybrid_format']['tokens']

print(f'Token Counts (10 chunks, ~500 chars each):')
print(f'  Plain:  {plain} tokens')
print(f'  TOON:   {toon} tokens (saves {plain - toon} tokens, {((plain - toon) / plain * 100):.1f}%)')
print(f'  Hybrid: {hybrid} tokens (saves {plain - hybrid} tokens, {((plain - hybrid) / plain * 100):.1f}%)')
print()
print(f'Cost Savings (at \$0.002 per 1K tokens):')
print(f'  Per query: \${((plain - toon) * 0.002 / 1000):.4f} saved with TOON')
print(f'  10K queries: \${((plain - toon) * 0.002 / 1000 * 10000):.2f} saved')
"
```

**Talking Points**:
- "For large contexts, TOON saves 40-50% tokens."
- "At scale, this translates to significant cost savings."
- "The savings increase with larger contexts."

---

### Step 7: Show Configuration Options (1 minute)

```bash
# Show how to change format
echo "Current format:"
docker-compose exec -T rag-service env | grep USE_LLM_TOKEN_FORMAT

echo ""
echo "To change format, update .env file or docker-compose.yml:"
echo "  USE_LLM_TOKEN_FORMAT=hybrid  # Recommended (best balance)"
echo "  USE_LLM_TOKEN_FORMAT=toon    # Maximum savings"
echo "  USE_LLM_TOKEN_FORMAT=plain   # No optimization (backward compatible)"
echo "  USE_LLM_TOKEN_FORMAT=json    # Structured format (for debugging)"
echo ""
echo "Then restart: docker-compose restart rag-service"
```

**Talking Points**:
- "Format is configurable via environment variable."
- "Default is `hybrid` - recommended for most use cases."
- "Can be changed without code changes - just restart service."

---

## One-Liner Quick Demo

If you only have 1 minute, run this:

```bash
cd rag_service && \
docker-compose exec -T rag-service python test_toon_docker_integration.py | \
  grep -A 20 "Test Large Context"
```

This shows the key benefit: **44%+ token savings for large contexts**.

---

## Key Demo Points to Emphasize

### âœ… What Works
1. **Automatic format selection**: Service automatically uses plain for small contexts, configured format for larger ones
2. **Significant token savings**: 40-50% savings for large contexts (10+ chunks)
3. **Production ready**: Fully tested, backward compatible, no breaking changes
4. **Configurable**: Easy to change format via environment variable
5. **Observable**: All format usage is logged with token counts

### ðŸ“Š Key Metrics to Show
- **Small contexts (3-5 chunks)**: TOON adds overhead â†’ auto-uses plain
- **Medium contexts (5-10 chunks)**: 5-10% savings with TOON
- **Large contexts (10+ chunks)**: **44%+ token savings** â­

### ðŸ’° Cost Impact
- **Per query**: ~$0.0003 saved (for large contexts)
- **10K queries/month**: ~$3 saved
- **100K queries/month**: ~$30 saved
- **1M queries/month**: ~$300 saved

### ðŸ”§ Technical Highlights
- Uses `tiktoken` for accurate token counting
- Falls back gracefully if tiktoken unavailable
- Logs all format usage for monitoring
- No impact on response quality
- Backward compatible (plain format still works)

---

## Troubleshooting During Demo

### If service is not running:
```bash
cd rag_service
docker-compose up -d
# Wait 30 seconds for health check
docker-compose ps
```

### If test script not found:
```bash
docker cp test_toon_docker_integration.py rag-service:/app/test_toon_docker_integration.py
```

### If format not being used:
```bash
# Check configuration
docker-compose exec rag-service env | grep USE_LLM_TOKEN_FORMAT

# Restart if needed
docker-compose restart rag-service
```

### If you need to show logs:
```bash
# Recent format events
docker-compose logs rag-service --tail 100 | grep "llm_context_formatted" | tail -5
```

---

## Demo Checklist

Before the demo:
- [ ] Docker services running and healthy
- [ ] Test script copied to container
- [ ] Current format verified (should be `hybrid`)
- [ ] Health endpoint responding

During the demo:
- [ ] Show service status
- [ ] Run comprehensive test (show large context savings)
- [ ] Show format comparison table
- [ ] Demonstrate real-time logging (optional)
- [ ] Explain configuration options

After the demo:
- [ ] Answer questions about token savings
- [ ] Explain when to use each format
- [ ] Show how to monitor in production

---

## Additional Resources

- **Test Report**: `TOON_TEST_REPORT.md` - Detailed test results
- **Integration Guide**: `TOON_INTEGRATION_GUIDE.md` - Technical details
- **Format Comparison**: `TOON_FORMAT_COMPARISON.md` - Format analysis
- **Demo Examples**: `TOON_DEMO.md` - Before/after examples

---

## Quick Reference Commands

```bash
# Check service status
docker-compose ps

# Check format configuration
docker-compose exec rag-service env | grep USE_LLM_TOKEN_FORMAT

# Run comprehensive test
docker-compose exec rag-service python test_toon_docker_integration.py

# Watch format usage in real-time
docker-compose logs -f rag-service | grep --line-buffered "llm_context_formatted"

# Make a test query
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "test query"}'

# View recent format events
docker-compose logs rag-service --tail 50 | grep "llm_context_formatted"
```

---

**Demo Duration**: 5-10 minutes  
**Key Takeaway**: TOON format provides 40-50% token savings for large contexts, reducing LLM costs significantly at scale.

