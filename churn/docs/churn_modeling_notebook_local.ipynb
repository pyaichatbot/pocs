{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merchant Churn Prediction \u2013 Modeling Notebook (Local Python + Snowflake Connector)\n",
        "\n",
        "This notebook assumes:\n",
        "\n",
        "1. You already created an **Analytical Base Table** (ABT) in Snowflake named `CHURN_MODEL_ABT`.\n",
        "2. Each row represents **one merchant at a snapshot date**.\n",
        "3. There is a binary target column, e.g. `IS_CHURNED_TARGET` (1 = churned, 0 = active).\n",
        "4. You have a `MERCHANT_ID` column used only as an identifier (not a feature).\n",
        "\n",
        "We will:\n",
        "- Connect to Snowflake (locally) using `snowflake-connector-python`\n",
        "- Load the ABT into pandas\n",
        "- Perform a time-based train/test split\n",
        "- Train a baseline Logistic Regression model\n",
        "- Train an XGBoost model\n",
        "- Evaluate with ROC-AUC and a simple lift analysis\n",
        "- Export a ranked churn risk report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install and import dependencies\n",
        "\n",
        "Run this cell once to install required libraries (if needed). Comment out `pip` lines once installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# !pip install snowflake-connector-python pandas scikit-learn xgboost matplotlib\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import snowflake.connector\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Snowflake connection\n",
        "\n",
        "Update the placeholders (`YOUR_...`) with your real values.\n",
        "Use environment variables in real projects for security.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "SNOWFLAKE_CONFIG = {\n",
        "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\", \"YOUR_ACCOUNT\"),\n",
        "    \"user\": os.getenv(\"SNOWFLAKE_USER\", \"YOUR_USER\"),\n",
        "    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\", \"YOUR_PASSWORD\"),\n",
        "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"YOUR_WAREHOUSE\"),\n",
        "    \"database\": os.getenv(\"SNOWFLAKE_DATABASE\", \"YOUR_DB\"),\n",
        "    \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\", \"YOUR_SCHEMA\"),\n",
        "    \"role\": os.getenv(\"SNOWFLAKE_ROLE\", \"YOUR_ROLE\"),\n",
        "}\n",
        "\n",
        "def get_snowflake_connection():\n",
        "    conn = snowflake.connector.connect(**SNOWFLAKE_CONFIG)\n",
        "    return conn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load ABT (`CHURN_MODEL_ABT`) from Snowflake\n",
        "\n",
        "Assumptions:\n",
        "- Table name: `CHURN_MODEL_ABT`\n",
        "- Contains at least:\n",
        "  - `MERCHANT_ID`\n",
        "  - `IS_CHURNED_TARGET` (0/1)\n",
        "  - Optional `SNAPSHOT_DATE` (for time-based split)\n",
        "  - Many engineered feature columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "conn = get_snowflake_connection()\n",
        "query = \"SELECT * FROM CHURN_MODEL_ABT\"\n",
        "df = pd.read_sql(query, conn)\n",
        "conn.close()\n",
        "\n",
        "print(\"Shape of ABT:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Basic sanity checks\n",
        "\n",
        "Check target balance and missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "target_col = \"IS_CHURNED_TARGET\"  # adjust if different\n",
        "id_cols = [\"MERCHANT_ID\"]  # adjust if you have more IDs\n",
        "\n",
        "print(df[target_col].value_counts(normalize=True).rename(\"ratio\"))\n",
        "print(\"\\nMissing values per column (top 20):\")\n",
        "print(df.isna().mean().sort_values(ascending=False).head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train/Test split (time-based if possible)\n",
        "\n",
        "If you have a `SNAPSHOT_DATE` column, we do a **temporal split**.\n",
        "Otherwise, we use a simple random split.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "date_col = \"SNAPSHOT_DATE\" if \"SNAPSHOT_DATE\" in df.columns else None\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in id_cols + [target_col]]\n",
        "\n",
        "if date_col:\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    cutoff_date = df[date_col].quantile(0.8)  # 80% train, 20% test by time\n",
        "    train_df = df[df[date_col] <= cutoff_date].copy()\n",
        "    test_df = df[df[date_col] > cutoff_date].copy()\n",
        "else:\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df[target_col]\n",
        "X_test = test_df[feature_cols]\n",
        "y_test = test_df[target_col]\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Handle class imbalance (optional)\n",
        "\n",
        "We start simple by using `class_weight='balanced'` in Logistic Regression.\n",
        "In later iterations, you can explore SMOTE or focal loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Simple numeric-only filter (if you have categorical features, encode them first)\n",
        "X_train_num = X_train.select_dtypes(include=[\"number\"]).fillna(0)\n",
        "X_test_num = X_test[X_train_num.columns].fillna(0)\n",
        "\n",
        "print(\"Numeric feature count:\", X_train_num.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Baseline model \u2013 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "lr.fit(X_train_num, y_train)\n",
        "y_pred_proba_lr = lr.predict_proba(X_test_num)[:, 1]\n",
        "\n",
        "roc_auc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_lr)\n",
        "pr_auc_lr = auc(recall, precision)\n",
        "\n",
        "print(f\"Logistic Regression ROC-AUC: {roc_auc_lr:.3f}\")\n",
        "print(f\"Logistic Regression PR-AUC:  {pr_auc_lr:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. XGBoost model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "xgb = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    tree_method=\"hist\",\n",
        ")\n",
        "\n",
        "xgb.fit(X_train_num, y_train)\n",
        "y_pred_proba_xgb = xgb.predict_proba(X_test_num)[:, 1]\n",
        "\n",
        "roc_auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, y_pred_proba_xgb)\n",
        "pr_auc_xgb = auc(recall_xgb, precision_xgb)\n",
        "\n",
        "print(f\"XGBoost ROC-AUC: {roc_auc_xgb:.3f}\")\n",
        "print(f\"XGBoost PR-AUC:  {pr_auc_xgb:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Simple lift analysis\n",
        "\n",
        "We rank merchants by predicted churn risk and see how many churners\n",
        "are captured in the top X% of the population.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "results = test_df[id_cols + [target_col]].copy()\n",
        "results[\"score_xgb\"] = y_pred_proba_xgb\n",
        "results = results.sort_values(\"score_xgb\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "results[\"decile\"] = pd.qcut(results.index, 10, labels=False) + 1  # 1 = top 10%\n",
        "\n",
        "lift_table = results.groupby(\"decile\").agg(\n",
        "    merchants=(target_col, \"count\"),\n",
        "    churners=(target_col, \"sum\"),\n",
        ").reset_index()\n",
        "\n",
        "lift_table[\"churn_rate\"] = lift_table[\"churners\"] / lift_table[\"merchants\"]\n",
        "\n",
        "print(lift_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export ranked churn risk report\n",
        "\n",
        "This CSV can be shared with business/AM teams for validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "output_cols = id_cols + [target_col, \"score_xgb\"]\n",
        "ranked_output = results[output_cols].sort_values(\"score_xgb\", ascending=False)\n",
        "output_path = \"churn_risk_scores_xgb.csv\"\n",
        "ranked_output.to_csv(output_path, index=False)\n",
        "output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. (Optional) Save the trained model\n",
        "\n",
        "In a real project, you would save the model to disk or a model registry\n",
        "for use in batch scoring or online inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(xgb, \"xgb_churn_model.pkl\")\n",
        "\"xgb_churn_model.pkl\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}