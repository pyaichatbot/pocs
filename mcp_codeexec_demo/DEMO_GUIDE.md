# MCP Code Execution Demo Guide

This guide walks you through demonstrating the filesystem-based code execution pattern and comparing it with direct tool calls.

## Prerequisites

- Docker and Docker Compose installed
- (Optional) LLM API keys for live demonstrations:
  - `ANTHROPIC_API_KEY` for Claude (local development)
  - `AZURE_API_KEY` + `AZURE_API_BASE` for Azure OpenAI (office/production)

## Quick Start

### 1. Start All Services

```bash
cd /Users/spy/Documents/PY/AI/pocs/mcp_codeexec_demo
docker-compose up --build
```

Wait for all services to start:
- ‚úÖ MCP Server: http://localhost:8974
- ‚úÖ Backend API: http://localhost:8000
- ‚úÖ Streamlit UI: http://localhost:8501

### 2. Open the UI

Navigate to **http://localhost:8501** in your browser.

You should see:
- Chat interface
- Toggle: "Filesystem-based Code Execution"
- Sidebar with topic and word count controls
- Token consumption chart (will populate after first message)

## Demo Scenarios

### Scenario 1: Direct Tool Calls (Inefficient) - Toggle OFF

**Purpose**: Demonstrate the traditional approach with high token usage.

#### Steps:

1. **Set the toggle to OFF** (Filesystem-based Code Execution = OFF)
   - You'll see: "**Mode:** üìù Direct Tool Calls"

2. **Send message**: `Create a summary`

#### What Happens:

- Backend discovers tools from MCP server
- Makes direct MCP tool call to fetch data
- **Full data loaded into model context** (all tool results)
- Tool definitions included in prompt
- Model processes entire dataset
- **Token Usage**: ~150,000+ tokens (depends on data size)

#### Expected Results:

- **Reply**: Executive summary generated from full transcript
- **Token Chart**: Shows high token consumption
- **Debug Info** (check API response):
  ```json
  {
    "strategy": "prompt_includes_full_data",
    "approx_chars": 15000+,
    "tokens_prompt": 150000+
  }
  ```

#### Key Points to Highlight:

- All tool definitions loaded upfront
- Full data blobs passed through model
- Intermediate results consume additional tokens
- Inefficient for large datasets

---

### Scenario 2: Filesystem-Based Code Execution (Efficient) - Toggle ON

**Purpose**: Demonstrate the efficient approach with ~98% token reduction.

#### Steps:

1. **Set the toggle to ON** (Filesystem-based Code Execution = ON)
   - You'll see: "**Mode:** üîß Code Execution (Filesystem)"

2. **Send message**: `Create a summary`

#### What Happens Behind the Scenes:

1. **Tool Discovery** (already done on startup):
   - Backend discovered MCP tools from connected server(s)
   - Generated Python files in `workspace/servers/{server_name}/`:
     - Tool files for each discovered tool
     - `__init__.py` for package structure

2. **Agent Code Generation**:
   - Agent explores `./servers/` directory
   - Discovers available tools dynamically
   - Generates Python code like:
     ```python
     from servers.{server_name} import {tool_name}
     
     # Fetch data using discovered tool
     result = await {tool_name}(...)
     
     # Process data in code (not in prompt)
     # ... data processing ...
     
     # Return only summary
     _result = {"summary": "..."}
     ```

3. **Code Execution** (with Phase 1 Security):
   - Generated code validated by security rules engine
   - Executed in sandboxed environment
   - Network egress controls enforced
   - File system restrictions enforced
   - Large data processed in code (not in prompt)
   - Only summary extracted and sent to LLM

4. **Final Processing**:
   - LLM receives only the processed result (not raw data)
   - Generates response from summary
   - All operations logged with correlation IDs

#### Expected Results:

- **Reply**: Executive summary (same quality, different process)
- **Token Chart**: Shows dramatically lower token consumption
- **Debug Info** (check API response):
  ```json
  {
    "strategy": "code_exec_filesystem",
    "generated_code": "from servers.demo_mcp import get_transcript...",
    "execution_success": true,
    "available_tools": ["demo_mcp"],
    "tokens_prompt": 2000
  }
  ```

#### Key Points to Highlight:

- ‚úÖ **Fully Generic**: Works with any MCP server and any tools
- ‚úÖ **Dynamic Discovery**: Tools discovered via filesystem (not hardcoded)
- ‚úÖ **Agent Code Generation**: Python code generated dynamically based on available tools
- ‚úÖ **Data Processing**: Large data processed in execution environment (not in prompt)
- ‚úÖ **Security**: Phase 1 security policies enforced (network, filesystem, code validation)
- ‚úÖ **Observability**: All operations logged with correlation IDs
- ‚úÖ **Token Usage**: ~2,000 tokens (~98% reduction vs direct tool calls)

---

### Scenario 3: Side-by-Side Comparison

**Purpose**: Visually demonstrate the token efficiency difference.

#### Steps:

1. **First, run with toggle OFF**:
   - Send: `Create executive summary`
   - Note the token count in the chart

2. **Then, run with toggle ON**:
   - Send: `Create executive summary` (same message)
   - Compare the token count

3. **Observe the chart**:
   - Two bars/lines showing token consumption
   - Clear visual difference between modes

#### Expected Comparison:

| Mode | Prompt Tokens | Output Tokens | Total | Reduction |
|------|---------------|---------------|-------|------------|
| Direct Tool Calls | ~150,000 | ~500 | ~150,500 | - |
| Code Execution | ~2,000 | ~500 | ~2,500 | **98.3%** |

---

## Example Questions to Trigger Tools

**Note:** This system is **fully generic** and works with **any MCP server** and **any tools**. The examples below are based on the demo MCP server which provides `get_transcript` and `update_crm` tools. Your MCP server may have different tools - the agent will discover and use them automatically.

This section provides a comprehensive list of questions you can ask in the UI to trigger different tools and demonstrate various capabilities.

### 1. Tool Discovery Requests ‚≠ê **Recommended First Step**

**Best for understanding what tools are available:**
- `What tools are available?`
- `List all available tools`
- `Show me what I can do`
- `What capabilities do you have?`

**What happens:**
- Agent explores `./servers/` directory
- Discovers all available tools from connected MCP servers
- Returns a list of tools with descriptions

### 2. Generic Data Fetching Requests

**The agent will discover and use appropriate tools automatically:**
- `Get data about sales`
- `Fetch information on quarterly revenue`
- `Retrieve customer feedback data`
- `Show me product launch information`

**With specific topics (if your MCP server supports topic parameters):**
- `Get data about market analysis`
- `Fetch information on payment processing`
- `Retrieve compliance and KYC data`

**Note:** The agent will automatically:
- Discover which tools can fetch data
- Use appropriate tool parameters
- Process results efficiently

### 3. Data Processing and Summarization Requests ‚≠ê **Best for Demonstrating Efficiency**

**Best for demonstrating code execution efficiency:**
- `Create a summary` ‚≠ê **Recommended for demos**
- `Summarize the data`
- `Give me a brief summary`
- `Draft an executive note`

**More specific:**
- `Create a crisp executive summary`
- `Generate a one-page summary`
- `Write an executive briefing`
- `Analyze the data and provide key insights`

**What happens:**
- Agent fetches data using discovered tools
- Processes large datasets in code execution environment
- Returns only summary to LLM (not full data)
- **Token efficiency**: ~98% reduction vs direct tool calls

### 4. Data Update/Write Requests

**The agent will discover and use appropriate update tools:**
- `Update record ABC123 with note: Customer requested refund`
- `Add a note to record XYZ789 about the meeting`
- `Save this information: Follow up next week`

**Combined workflows:**
- `Get the data, analyze it, and save the summary to record 12345`
- `Create a summary and save it to record ABC123`
- `Fetch data, summarize it, and update the system with the summary`

**Note:** Tool names and parameters are discovered dynamically - no hardcoding!

### 5. Complex Multi-Step Workflows

**Best for demonstrating state persistence, skills, and tool orchestration:**
- `Get the data, analyze it, and save the key points to a file`
- `Fetch data, create a summary, and save it to a record`
- `Get multiple datasets on different topics and compare them`
- `Fetch data, extract the main themes, and save them as a CSV`
- `Process the data and create a report with visualizations`

**What happens:**
- Agent discovers multiple tools
- Chains tool calls together
- Uses workspace for intermediate state
- Processes data efficiently in code execution

### Recommended Demo Flow

#### Step 1: Tool Discovery (Code Execution Mode ON) ‚≠ê **Start Here**
```
What tools are available?
```
- **Shows**: Dynamic tool discovery from MCP server
- **Demonstrates**: Generic approach - no hardcoded tools
- **Result**: List of all available tools from your MCP server

#### Step 2: Simple Data Fetch (Code Execution Mode ON)
```
Get data about sales
```
- **Shows**: Agent discovers and uses appropriate tool automatically
- **Demonstrates**: Dynamic tool selection based on user request
- **Result**: Data fetched using discovered tools

#### Step 3: Data Processing (Code Execution Mode ON) ‚≠ê **Best for Efficiency Demo**
```
Create a summary
```
- **Shows**: Token efficiency (data processed in code, not in prompt)
- **Demonstrates**: Large datasets processed in execution environment
- **Result**: Summary generated with minimal token usage
- ‚≠ê **This is the best question for demonstrating the efficiency difference**

#### Step 4: Compare Modes
- Same prompt with toggle **OFF**: High token usage (direct tool calls)
- Same prompt with toggle **ON**: Low token usage (code execution)
- **Shows**: Visual comparison in token chart (~98% reduction)

#### Step 5: Multi-Tool Workflow (Code Execution Mode ON)
```
Get data, analyze it, and save the summary
```
- **Shows**: Multi-tool orchestration
- **Demonstrates**: Agent chains multiple tools together
- **Result**: Complete workflow executed efficiently

### Tips for Effective Demos

1. **Start with "What tools are available?"** - Shows the generic, dynamic nature of the system.

2. **Use "Create a summary" as your primary demo question** - It clearly shows the efficiency difference between modes.

3. **The system is fully generic** - Works with any MCP server and any tools. No hardcoded assumptions!

4. **Try multi-step requests** to demonstrate state persistence, tool chaining, and agent orchestration.

5. **Compare both modes** with the same question to highlight the token efficiency difference visually (~98% reduction).

6. **Customize for your MCP server** - The examples use generic language. Adapt based on your actual tools.

### What Happens Behind the Scenes

**In Code Execution Mode (Toggle ON):**
1. **Tool Discovery**: Agent explores `./servers/` directory to find all available tools
2. **Dynamic Tool Selection**: Agent identifies which tools match the user's request
3. **Code Generation**: Agent generates Python code: `from servers.{server_name} import {tool_name}`
4. **Code Execution**: Executes code in sandboxed environment with security policies
5. **Data Processing**: Processes large data in code (not in prompt)
6. **Efficient Return**: Returns only processed results to LLM (not raw data)

**In Direct Tool Calls Mode (Toggle OFF):**
1. **Direct Tool Call**: Makes direct MCP tool call
2. **Full Data in Prompt**: All tool results loaded into model context
3. **High Token Usage**: Every byte of data consumes tokens

**Key Insights**: 
- **Generic Approach**: No hardcoded tools - works with any MCP server
- **Dynamic Discovery**: Tools discovered at runtime from filesystem
- **Efficiency**: Ask for tasks that require data processing (like summaries) to see the biggest token efficiency difference (~98% reduction)
- **Security**: All code execution protected by Phase 1 security policies

---

## Advanced Demo: Inspecting Generated Files

### View Generated Tool Files

```bash
# List all server directories (each MCP server gets its own directory)
docker-compose exec backend ls -la /app/workspace/servers/

# List tools for a specific server (replace {server_name} with actual server name)
docker-compose exec backend ls -la /app/workspace/servers/{server_name}/

# View a generated tool file (replace with actual server and tool names)
docker-compose exec backend cat /app/workspace/servers/{server_name}/{tool_name}.py
```

### Check Backend Logs

```bash
# View tool generation on startup
docker-compose logs backend | grep -E "(‚úÖ|üîß|Generated)"

# View code execution logs
docker-compose logs backend | grep -E "(code_exec|execution)"
```

### Test API Directly

```bash
# Test code execution mode
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Create a summary",
    "mode": "code_exec"
  }' | python -m json.tool

# Test direct tool calls mode
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Create a summary",
    "mode": "prompt_tools"
  }' | python -m json.tool

# Test tool discovery
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What tools are available?",
    "mode": "code_exec"
  }' | python -m json.tool
```

---

## Demo Script for Presentations

### Introduction (2 minutes)

1. **Context**: 
   - "This demo shows how agents can interact with MCP servers more efficiently"
   - "We'll compare two approaches: direct tool calls vs filesystem-based code execution"

2. **Problem Statement**:
   - "Traditional approach: Load all tool definitions and data into model context"
   - "Result: High token usage, especially with large datasets"

### Part 1: Tool Discovery (2 minutes)

1. **Show toggle ON**
2. **Send message**: "What tools are available?"
3. **Highlight**:
   - Generic approach - no hardcoded tools
   - Dynamic discovery from MCP server
   - Tools presented as Python files

### Part 2: Direct Tool Calls (3 minutes)

1. **Show toggle OFF**
2. **Send message**: "Create a summary"
3. **Highlight**:
   - Full data loaded into context
   - Tool definitions included
   - High token usage (~150,000 tokens)

### Part 3: Code Execution (5 minutes)

1. **Show toggle ON**
2. **Explain what happens**:
   - "Tools are generated as Python files on startup from any MCP server"
   - "Agent discovers tools dynamically via filesystem"
   - "Agent generates Python code to interact with discovered tools"
   - "Security policies enforce network and filesystem restrictions"
3. **Send same message**: "Create a summary"
4. **Highlight**:
   - Code generated dynamically based on available tools
   - Data processed in execution environment (not in prompt)
   - Security policies enforced (Phase 1)
   - Only summary sent to model
   - Low token usage (~2,000 tokens)
   - **~98% reduction**

### Part 4: Comparison (2 minutes)

1. **Show token chart** with both modes
2. **Highlight the difference**:
   - Same quality output
   - 98.7% token reduction
   - More scalable approach

### Conclusion (1 minute)

- **Key Takeaway**: Generic filesystem-based code execution enables efficient agent interactions with any MCP server
- **Benefits**:
  - **Generic Architecture**: Works with any MCP server and any tools
  - **Progressive Disclosure**: Tools discovered on-demand via filesystem
  - **Code Execution**: Data processing in secure execution environment
  - **Security**: Phase 1 policies enforce network, filesystem, and code validation
  - **Observability**: Full logging with correlation IDs for troubleshooting
  - **State Persistence**: Workspace maintains state across operations
  - **Token Efficiency**: ~98% reduction vs direct tool calls

---

## Troubleshooting

### Services Not Starting

```bash
# Check service status
docker-compose ps

# View logs
docker-compose logs backend
docker-compose logs mcp
docker-compose logs ui
```

### Tool Files Not Generated

```bash
# Manually trigger tool generation
docker-compose exec backend python -c "
import asyncio
from tool_generator import generate_tool_files
result = asyncio.run(generate_tool_files())
print(result)
"
```

### LLM Errors

If you see "LLM Error" in responses, the system will fallback to stubbed responses (demo still works, but without real LLM).

#### Common Error Types:

**1. "Model not found" (NotFoundError)**
```
Model 'claude-3-5-sonnet-20241022' not found. Check:
1. Model name is correct (e.g., 'claude-3-5-sonnet-20241022')
2. API key has access to this model
3. API endpoint is correct
```

**Solutions:**
- **Anthropic**: Verify the model name. Common models:
  - `claude-3-5-sonnet-20241022` (default)
  - `claude-3-opus-20240229`
  - `claude-3-sonnet-20240229`
  - `claude-3-haiku-20240307`
- **Azure OpenAI**: Check model deployment name matches `MODEL_ID`
- **OpenAI**: Verify model name (e.g., `gpt-4o`, `gpt-4o-mini`)

**2. "Authentication failed"**
```
Authentication failed. Check:
1. API key is valid and not expired
2. API key format is correct
```

**Solutions:**
- Verify API key is set: `docker-compose exec backend env | grep API_KEY`
- Check API key format:
  - Anthropic: Should start with `sk-ant-`
  - OpenAI/Azure: Should start with `sk-`
- Regenerate API key if expired
- For Azure: Ensure `AZURE_API_BASE` is set correctly

**3. General LLM Errors**

**Check backend logs:**
```bash
docker-compose logs backend | grep -i "error\|llm"
```

**Verify environment variables:**
```bash
# Check what's set
docker-compose exec backend env | grep -E "API_KEY|MODEL|AZURE"

# For Anthropic
docker-compose exec backend python3 -c "import os; print('ANTHROPIC_API_KEY:', 'SET' if os.getenv('ANTHROPIC_API_KEY') else 'NOT SET')"

# For Azure
docker-compose exec backend python3 -c "import os; print('AZURE_API_KEY:', 'SET' if os.getenv('AZURE_API_KEY') else 'NOT SET'); print('AZURE_API_BASE:', os.getenv('AZURE_API_BASE', 'NOT SET'))"
```

**Test API key directly:**
```bash
# Test Anthropic API key
curl https://api.anthropic.com/v1/messages \
  -H "x-api-key: YOUR_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "content-type: application/json" \
  -d '{"model":"claude-3-5-sonnet-20241022","max_tokens":10,"messages":[{"role":"user","content":"test"}]}'
```

**Note**: The demo will still work with stubbed responses even if LLM fails. Stubbed responses allow you to demonstrate the architecture and token efficiency concepts without requiring a valid API key.

### UI Not Loading

- Check http://localhost:8501 is accessible
- Check backend is running: http://localhost:8000/docs
- Check MCP server: http://localhost:8974/mcp

---

## Key Metrics to Highlight

### Token Efficiency

- **Direct Tool Calls**: 150,000+ tokens
- **Code Execution**: 2,000 tokens
- **Reduction**: 98.7%

### Architecture Benefits

1. **Progressive Disclosure**: Tools discovered on-demand via filesystem
2. **Code Execution**: Data processing happens in execution environment
3. **State Persistence**: Workspace maintains state across sessions
4. **Scalability**: Works with large datasets without token bloat

---

## Next Steps

After the demo, you can:

1. **Explore the code**:
   - `backend/tool_generator.py` - Tool discovery and file generation
   - `backend/code_executor.py` - Code execution environment
   - `backend/main.py` - Main API with both modes

2. **Extend the demo**:
   - Add more MCP tools
   - Customize tool generation
   - Add more complex code execution scenarios

3. **Read the blog post**:
   - [Code execution with MCP](https://www.anthropic.com/engineering/code-execution-with-mcp)

---

## Summary

This demo successfully demonstrates:

‚úÖ **Filesystem-based tool discovery** - Tools as Python files  
‚úÖ **Agent code generation** - Dynamic Python code creation  
‚úÖ **Sandboxed execution** - Safe code execution environment  
‚úÖ **Token efficiency** - 98.7% reduction in token usage  
‚úÖ **Progressive disclosure** - On-demand tool discovery  
‚úÖ **State persistence** - Workspace maintains state  

The implementation fully matches the architecture described in the Anthropic blog post.

